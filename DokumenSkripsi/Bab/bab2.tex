%versi 2 (8-10-2016)
\chapter{Landasan Teori}
\label{chap:teori}

\section{\textit{Big Data}}
\label{sec:big data}

\textit{Big data} adalah istilah yang menggambarkan kumpulan data dalam jumlah yang sangat besar, baik data yang terstruktur maupun data yang tidak terstruktur. Kumpulan data tersebut menyimpan informasi yang bisa dianalisis dan diproses untuk memberikan wawasan kepada organisasi atau perusahaan. Data-data tersebut berasal dari satu atau lebih sumber  dengan kecepatan yang tinggi dan \textit{format} yang berbeda-beda. Karena ukurannya dan keberagaman data, \textit{big data} menjadi sulit untuk ditangani atau diproses jika hanya menggunakan manajemen basis data atau aplikasi pemrosesan data tradisional~\cite{ishwarappa:01:bgintro}.\\


\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.4]{bigdata}  
    \caption[Karakteristik \textit{big data} ]{Karakteristik \textit{big data}} 
    \label{fig:bigdata} 
\end{figure}

Berdasarkan Gambar ~\ref{fig:bigdata}, \textit{big data} memiliki lima karakteristik di antaranya ~\cite{ishwarappa:01:bgintro}:

\begin{enumerate}

\item \textit{Volume}: \textit{big data} memiliki jumlah data yang sangat besar sehingga dalam proses pengolahan data dibutuhkan suatu penyimpanan yang besar dan dibutuhkan analisis yang lebih spesifik.

\item \textit{Velocity}: \textit{big data} memiliki aliran data yang sangat cepat. Data baru dihasilkan dengan kecepatan yang tinggi dari satu atau lebih sumber. 

\item \textit{Variety}: \textit{big data} memiliki bentuk format data yang beragam, baik terstruktur ataupun tidak terstruktur dan bergantung pada banyaknya sumber data. Data dapat berupa gambar, video dan tipe data lainnya.

\item \textit{Veracity}: \textit{big data} dapat mengandung data yang tidak akurat atau rusak. Kualitas  data dalam \textit{big data} bisa berbeda-beda bergantung pada sumber. Analisis \textit{big data} akan sangat dipengaruhi dengan keakuratan data.

\item \textit{Value}: \textit{big data} harus memiliki \textit{value}. Tidak ada gunanya bila kita memiliki akses terhadap \textit{big data}, tetapi data-data tersebut tidak memiliki nilai apapun. Data yang tidak memiliki nilai adalah data yang tidak berguna dan memakan biaya untuk disimpan.

\end{enumerate}

\textit{Big data} sangat bermanfaat ketika diterapkan di berbagai macam bidang seperti bisnis, kesehatan, pemerintahan, pertanian dan lainya. Ketika organisasi mampu menggabungkan jumlah data besar yang dimilikinya dengan analisis bertenaga tinggi, organisasi dapat menyelesaikan tantangan dan masalah yang berhubungan dengan bisnis seperti:

\begin{enumerate}

\item Menentukan akar penyebab kegagalan untuk setiap masalah bisnis.

\item Menghasilkan informasi mengenai titik penting penjualan berdasarkan kebiasaan pelanggan dalam membeli.

\item Menghitung kembali seluruh risiko yang ada dalam waktu yang singkat.

\item Mendeteksi perilaku penipuan yang dapat mempengaruhi organisasi.

\end{enumerate}

\section{Algoritma Hierarchical Clustering}

\textit{Hierarchical Clustering Algorithm} (HCA) adalah metode analisis kelompok yang berusaha untuk membangun sebuah hierarki dengan mengelompokkan data. Dengan mengelompokkan data-data tersebut, data pada kelompok yang sama memiliki kemiripan yang tinggi dan data pada kelompok yang berbeda memiliki kemiripan yang rendah~\cite{veronica:02:bdhca}. Dalam reduksi data, \textit{cluster} yang merepresentasikan data-data pada \textit{cluster} tersebut akan digunakan untuk mengganti data-data mentah~\cite{veronica:02:bdhca}. Seberapa efektif cara ini bergantung pada sifat data yang ditangani. Data-data yang bisa dikelompokkan ke dalam \textit{cluster} yang berbeda akan sangat cocok dengan cara ini~\cite{veronica:02:bdhca}. Pada dasarnya HCA dibagi menjadi dua jenis, yaitu \textit{agglomerative} (\textit{bottom-up}) dan \textit{devisive} (\textit{top-down})~\cite{veronica:02:bdhca}. Pendekatan \textit{agglomerative} berusaha membentuk sebuah hierarki dengan menggabungkan \textit{cluster}. Setiap objek akan dimasukkan kepada \textit{cluster} tersendiri. Sebaliknya, pendekatan \textit{devisive} akan berusaha memecah \textit{cluster} untuk membentuk sebuah hierarki. Setiap objek berada pada satu \textit{cluster} pada awalnya dan akan dipecah kepada \textit{cluster} yang berbeda.\\

\begin{figure}[H]
    \centering  
    \includegraphics[scale=1]{matrix1}  
    \caption[Matriks jarak]{Matriks jarak} 
    \label{fig:matrix1} 
\end{figure}

\begin{figure}[H]
    \centering  
    \includegraphics[scale=1]{matrix2}  
    \caption[Matriks jarak]{Matriks jarak} 
    \label{fig:matrix2} 
\end{figure}

Pada \textit{Hierarchical Agglomerative Clustering}, awalnya setiap objek akan dimasukkan kepada \textit{cluster} tersendiri. Matriks jarak digunakan untuk merepresentasikan jarak antara \textit{cluster}. Kemudian, dua buah \textit{cluster} yang memiliki jarak terdekat akan digabungkan menjadi satu \textit{cluster}. Jarak antara cluster dapat dihitung dengan tiga metode, yaitu \textit{single linkage}, \textit{complete linkage}, dan \textit{centroid linkage}~\cite{anil:03:afcd}. Pada Gambar ~\ref{fig:matrix1}, \textit{cluster} A dan  \textit{cluster} B akan digabung menjadi satu karena jarak antara keduanya adalah terkecil dibanding dengan yang lainnya. Gambar ~\ref{fig:matrix2} adalah hasil dari penggabungan \textit{cluster} A dan \textit{cluster} B. Kemudian, matriks jarak perlu dihitung kembali untuk mencari jarak baru antara \textit{cluster} baru dengan \textit{cluster} lainya. Penggabungan \textit{cluster} akan diulangi sampai tersisa satu \textit{cluster}. \textit{Hierarchical Agglomerative Clustering} akan hanya membutuhkan maksimal $n$ iterasi. Hasil dari penggabungan \textit{cluster} adalah sebuah hierarki. \textit{Dendrogram} sangat umum digunakan untuk menggambarkan proses \textit{Hierarchical Agglomerative Clustering}. Contoh \textit{dendrogram} dapat dilihat pada Gambar ~\ref{fig:dendo}.\\


\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45]{dendo}  
    \caption[\textit{dendrogram} ]{\textit{dendrogram}} 
    \label{fig:dendo} 
\end{figure}


Berikut adalah penjelasan mengenai metode \textit{single linkage}, \textit{complete linkage}, dan \textit{centroid linkage}:

\begin{itemize}

\item \textit{Single linkage}: metode ini mencari jarak minimum dari perbandingan setiap anggota antara dua buah \textit{cluster}. Bila terdapat \textit{cluster} A dan \textit{cluster} B, maka setiap anggota pada \textit{cluster} A akan dihitung jaraknya kepada setiap anggota pada \textit{cluster} B. Kemudian jarak minimum antara anggota akan diambil sebagai hasilnya. Untuk menghitung jarak antara anggota dapat digunakan \textit{euclidean distance}, \textit{manhattan distance}, atau ruang metrik lainnya. Ruang metrik yang digunakan disesuaikan dengan kebutuhan dan atribut dari data. Contoh \textit{single linkage} dapat dilihat pada Gambar ~\ref{fig:single}. 

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45]{single}  
    \caption[Metode \textit{single linkage} ]{Metode \textit{single linkage}} 
    \label{fig:single} 
\end{figure}

Rumus~\ref{eq:eq1} adalah rumus untuk \textit{single linkage}:

\begin{equation} \label{eq:eq1}
min\{ d(a,b): a \in A, b \in B\}, 
\end{equation}

dengan $a$ dan $b$ merupakan anggota dari \textit{cluster} A dan B.\\

\item \textit{Complete linkage}: metode ini adalah kebalikan dari metode \textit{single linkage}. Bila terdapat \textit{cluster} A dan B, maka setiap anggota pada \textit{cluster} A akan dihitung jaraknya kepada setiap anggota pada \textit{cluster} B. Kemudian jarak maksimum antara anggota akan diambil sebagai hasilnya. Contoh  \textit{complete linkage} dapat dilihat pada Gambar ~\ref{fig:complete}.

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.45]{complete}  
    \caption[Metode \textit{complete linkage} ]{Metode \textit{complete linkage}} 
    \label{fig:complete} 
\end{figure}

Rumus~\ref{eq:eq2} adalah rumus untuk \textit{complete linkage}:

\begin{equation} \label{eq:eq2}
max\{ d(a,b): a \in A, b \in B\},
\end{equation}

dengan $a$ dan $b$ merupakan anggota dari \textit{cluster} $A$ dan $B$.\\


\item \textit{Centroid linkage}: metode ini menghitung jarak antara \textit{centroid} dari dua buah \textit{cluster}.  \textit{Centroid} merupakan titik tengah dari sebuah \textit{cluster}. \textit{Centroid} sebuah \textit{cluster} didapatkan dengan menghitung rata-rata dari setiap atribut dari anggota pada \textit{cluster}. Contoh  \textit{centroid linkage} dapat dilihat pada Gambar~\ref{fig:centroid}. 

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.5]{centroid}  
    \caption[Metode \textit{centroid linkage} ]{Metode \textit{centroid linkage}} 
    \label{fig:centroid} 
\end{figure}

Rumus~\ref{eq:eq3} adalah rumus untuk \textit{centroid linkage}:

\begin{equation} \label{eq:eq3}
\| c_{a} - c_{b} \|, 
\end{equation}

dengan $c_{a}$ dan $c_{b}$ merupakan \textit{centroid} dari \textit{cluster} $A$ dan $B$.\\


\end{itemize}








%%%%%PINDAHKAN KE BAB 4!!!!!

\begin{table}[H] 
	\centering 
	\caption{Tabel Data Koordinat}
	\label{tab:data}
	\begin{tabular}{|p{1.5cm}|p{1cm}|p{1cm}|}

\hline
 Cluster & x & y \\
\hline
A & 2 & 2 \\
\hline
B & 2 & 3 \\
\hline
C & 4 & 6 \\
\hline
D & 8 & 10 \\
\hline

	\end{tabular} 
\end{table}

Sebagai contoh, diberikan data yang memiliki atribut berupa koordinat $x$ dan $y$. Data dapat dilihat pada Tabel~\ref{tab:data}. Data tersebut akan diolah dengan algoritma \textit{Hierarchical Agglomerative Clustering} menggunakan metode \textit{single linkage} dan \textit{euclidean distance} untuk menghitung jaraknya antara anggotanya. Berikut adalah langkah-langkah penyelesaiannya.



\begin{enumerate}

\item Pertama, hitung matriks jarak antara \textit{cluster}. Karena setiap \textit{cluster} hanya memiliki satu anggota pada awalnya, Jarak antara \textit{cluster} dapat langsung dihitung  menggunakan \textit{euclidean distance}. Matriks jarak yang hasilkan bisa dilihat pada Gambar~\ref{fig:step1}.

\begin{figure}[H]
    \centering  
    \includegraphics[scale=1]{step1}  
    \caption[Matriks jarak]{Matriks jarak} 
    \label{fig:step1} 
\end{figure}

Jarak antara \textit{cluster} A dan \textit{cluster} B dapat dihitung dengan cara berikut:





\begin{equation}
\begin{split}
d & = \sqrt{(x_{1} - x_{2})^2+(y_{1} - y_{2})^2} \\
& = \sqrt{(2 - 2)^2+(2 - 3)^2} \\
& = \sqrt{0 + 1} \\
& = \sqrt{1} \\
& = 1 
\end{split}
\end{equation}


\item Selanjutnya, gabungkan dua \textit{cluster} yang memiliki jarak terdekat. Pada contoh ini,  \textit{cluster} A yang dibandingkan terhadap \textit{cluster} B memiliki nilai terkecil yaitu $1$. Jarak antara kedua \textit{cluster} adalah yang terdekat. Hasil dari penggabungan kedua cluster dapat dilihat pada Gambar~\ref{fig:step2}.

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.8]{step2}  
    \caption[Hasil penggabungan \textit{cluster}]{Hasil penggabungan \textit{cluster}} 
    \label{fig:step2} 
\end{figure}

\item Setelah itu, matriks jarak harus dihitung ulang untuk mencari jarak antara \textit{cluster} barunya, yaitu (A,B) dengan yang lainya. Untuk menghitung ulang antara \textit{cluster} baru dengan \textit{cluster} lainnya, digunakan metode \textit{single linkage}. Pada tahap ini setiap anggota dari cluster (A,B) akan dihitung jaraknya terhadap \textit{cluster} C dan \textit{cluster} D. Nilai minimum akan diambil sebagai hasil perbandingannya karena metode yang digunakan adalah \textit{single linkage}. Berikut adalah contoh perhitungan antara \textit{cluster} (A,B) dengan \textit{cluster} C menggunakan metode \textit{single linkage}.


\begin{equation} 
\begin{split}
d(A,C) & = \sqrt{(2 - 2)^2+(4 - 6)^2} \\
& = 4.47 
\end{split}
\end{equation}

\begin{equation} 
\begin{split}
d(B,C) & = \sqrt{(2 - 3)^2+(4 - 6)^2} \\
& = 3.61 
\end{split}
\end{equation}


Berdasarkan perhitungan di atas, nilai 3.61 diambil sebagai hasil karena nilai tersebut lebih kecil dibandingkan 4.47. Contoh hasil dapat dilihat pada Gambar~\ref{fig:step3}. 

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.8]{step3}  
    \caption[Hasil rekalkulasi]{Hasil rekalkulasi} 
    \label{fig:step3} 
\end{figure}

\item Ulangi langkah 2 dan 3 sampai satu \textit{cluster} yang tersisa. Hasil akhir dalam bentuk \textit{dendrogram} dapat dilihat pada Gambar~\ref{fig:hasild}.

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.6]{hasild}  
    \caption[Hasil akhir \textit{dendrogram} ]{Hasil akhir \textit{dendrogram}} 
    \label{fig:hasild} 
\end{figure}

\end{enumerate}

Setelah \textit{dendrogram} terbentuk, \textit{dendrogram} perlu dipotong berdasarkan nilai \textit{cut-off distance} yang ditentukan. Nilai \textit{cut-off distance} menentukan banyaknya \textit{cluster} yang dihasilkan ketika memotong \textit{dendrogram}. Semakin tinggi nilai \textit{cut-off distance}, semakin sedikit \textit{cluster} yang dihasilkan dan sebaliknya. Berdasarkan Gambar~\ref{fig:hasild}, dapat dilihat bahwa nilai \textit{cut-off distance} yang lebih tinggi menghasilkan \textit{cluster-cluster} yang lebih sedikit. Sedangkan, nilai \textit{cut-off distance} yang lebih rendah menghasilkan \textit{cluster-cluster} yang lebih banyak. Perpotongan akan berdampak kepada hasil akhir ukuran data yang dihasilkan. 

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.5]{codis}  
    \caption[Perpotongan \textit{dendrogram} ]{Perpotongan \textit{dendrogram}} 
    \label{fig:codis} 
\end{figure}

Dari setiap \textit{cluster} yang dihasilkan dari perpotongan, perlu dicari jumlah anggota pada \textit{cluster}, nilai minimum, maksimum, rata-rata, dan standar deviasi dari setiap atribut. Nilai-nilai tersebut dapat disebut sebagai pola. Pola ini akan merepresentasikan dan menggambarkan karakteristik \textit{cluster} tersebut. Pola ini akan disimpan sebagai hasil akhir untuk menggantikan data aslinya. Sebagai contoh diberikan sebuah \textit{cluster} A yang memiliki 4 anggota pada \textit{cluster}-nya. Setiap anggota memiliki 2 nilai atribut yang berbeda. Data untuk \textit{cluster} A dapat dilihat pada Tabel~\ref{tab:cont1}. Data pada tabel ini akan digunakan untuk mencari pola untuk \textit{cluster} A.


\begin{table}[H]
    \centering 
    \caption{Tabel Contoh Data \textit{Cluster}}
    \label{tab:cont1}
\begin{tabular}{|p{2cm}|p{2.2cm}|p{2.5cm}|}
\hline
\multicolumn{3}{|l|}{Cluster A}  \\
\hline
No & atribut 1 & atribut 2     \\
\hline
1  & 2         & 1             \\
\hline
2  & 4         & 5             \\
\hline
3  & 5         & 10            \\
\hline
4  & 6         & 7            \\
\hline
\end{tabular}
\end{table}

Hasil pola dari \textit{cluster} A dapat dilihat pada Tabel~\ref{tab:cont2}.



\begin{table}[H]
\centering 
    \caption{Tabel Hasil Pola Cluster A}
    \label{tab:cont2}
\begin{tabular}{|p{5cm}|p{2.5cm}|p{2.5cm}|}
\hline
jumlah anggota pada cluster & \multicolumn{2}{l|}{4}  \\
\hline
                            & atribut 1 & atribut 2  \\
\hline
nilai minimum               & 2         & 1          \\
\hline
nilai maksimum              & 6         & 10          \\
\hline
nilai rata-rata             & 4.25      & 5.75        \\
\hline
standar deviasi             & 1.479        & 3.269 \\   
\hline     
\end{tabular}
\end{table}




\section{Hadoop}

Hadoop dikembangkan oleh Doug Cutting dan Mike Cafarella pada tahun 2005 yang saat itu bekerja di Yahoo. Nama Hadoop diberikan berdasarkan mainan 'Gajah' anak dari Doug Cutting. Hadoop adalah sebuah \textit{framework} atau platform \textit{open source} berbasis Java. Hadoop memiliki kemampuan untuk menyimpan dan memproses data dengan skala yang besar secara terdistribusi pada \textit{cluster}. \textit{Cluster} tersebut terdiri atas perangkat keras komoditas~\cite{alexholmes:04:hip}. Hadoop menggunakan teknologi Google MapReduce dan  \textit{Google File System} (GFS) sebagai fondasinya~\cite{tomwhite:05:htdg}. Beberapa karakteristik yang dimiliki Hadoop adalah sebagai berikut:


\begin{enumerate}

\item \textit{Open Source}: Hadoop merupakan proyek \textit{open source} dan kodenya bisa dimodifikasi sesuai kebutuhan. 

\item \textit{Distributed Computing}: Data disimpan secara terdistribusi pada \textit{Hadoop Distributed File System} (HDFS) dan data dapat diproses secara paralel pada \textit{node-node} di \textit{cluster}.

\item \textit{Fast}: Hadoop sangat cocok untuk melakukan \textit{batch processing} bervolume besar karena mampu melakukannya secara paralel.

\item \textit{Fault Tolerance}: Hadoop melakukan duplikasi data di beberapa \textit{node} yang berbeda. Ketika sebuah node gagal memproses data, \textit{node} yang memiliki duplikat data dapat menggantikannya untuk memproses data tersebut.

\item \textit{Reliability}: Kegagalan mesin bukan masalah bagi Hadoop karena adanya duplikasi data.


\item \textit{High Availability}: Data dapat diambil dari sumber yang lain meskipun kegagalan mesin karena adanya duplikasi data.

\item \textit{Scalability}: Hadoop dapat menambahkan node yang lebih banyak ke dalam \textit{cluster} dengan mudah.

\item \textit{Flexibility}: Hadoop dapat menangani data terstruktur maupun data tidak terstruktur. 

\item \textit{Economic And Cost Effective}: Hadoop tidak mahal karena berjalan pada \textit{cluster} yang terdiri atas perangkat keras komoditas.

\item \textit{Easy To Use}: Hadoop mempermudah pengguna dalam merancang program paralel. Hadoop sudah menangani pembagian dan penugasan kerjaan secara paralel.

\item \textit{Data Locality}: Algoritma MapReduce akan didekatkan kepada \textit{cluster} dan tidak sebaliknya. Ukuran data yang besar lebih sulit untuk dipindahkan dibanding ukuran algoritma yang kecil.


\end{enumerate}

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.4]{hmodule}  
    \caption[Modul-modul Hadoop ]{Modul-modul Hadoop} 
    \label{fig:hmodule} 
\end{figure}

Berdasarkan Gambar~\ref{fig:hmodule}, \textit{framework} Apache Hadoop terdiri dari beberapa modul. Modul-modul tersebut membentuk dan membantu pemrosesan data berskala besar. Modul-modul tersebut di antaranya adalah~\cite{tomwhite:05:htdg}:

\begin{enumerate}
\item \textit{Hadoop Common}: modul ini terdiri atas \textit{library} dan \textit{tools} yang dibutuhkan module Hadoop lainnya.

\item \textit{Hadoop Distributed File System} (HDFS): sebuah file sistem terdistribusi milik Hadoop untuk penyimpanan data.

\item \textit{Hadoop YARN}: \textit{resource-management platform} yang bertanggung jawab untuk mengatur sumber daya pada \textit{cluster}.

\item MapReduce: sebuah model pemrograman untuk pemrosesan skala besar.

\end{enumerate}

\subsection{Hadoop Distributed File System (HDFS)}

Hadoop Distributed File System (HDFS) adalah sistem file terdistribusi yang dirancang untuk berjalan pada perangkat keras komoditas~\cite{tomwhite:05:htdg}. HDFS berbeda dari file sistem terdistribusi lainnya  karena sifat \textit{fault tolerance} yang tinggi dan dirancang untuk digunakan pada perangkat keras biasa. HDFS menyediakan akses \textit{throughput} yang tinggi dan cocok untuk \textit{data set} yang besar. HDFS awalnya dibangun sebagai infrastruktur untuk proyek mesin pencari web Apache Nutch.\\

Kegagalan perangkat keras sudah biasa terjadi. HDFS mungkin terdiri atas ratusan atau ribuan mesin server, masing-masing menyimpan bagian data dari file sistem. Faktanya, ada sejumlah besar komponen dan setiap komponen memiliki probabilitas kegagalan. Hal ini  menandakan bahwa terdapat beberapa komponen HDFS selalu tidak berfungsi. Oleh karena itu, deteksi kesalahan dan pemulihan otomatis yang cepat dari sistem adalah tujuan arsitektur inti dari HDFS.\\

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.6]{hdfshadoop}  
    \caption[Arsitektur HDFS]{Arsitektur HDFS} 
    \label{fig:hdfshadoop} 
\end{figure}

Hadoop meimplementasikan arsitektur \textit{Master Slave} pada komponen primernya yaitu HDFS dan MapReduce ~\cite{tomwhite:05:htdg}. Berdasarkan  (Gambar ~\ref{fig:hdfshadoop}), \textit{master} \textit{node} atau disebut NameNode bertugas untuk mengatur operasi-operasi seperti membuka, menutup, dan menamakan kembali file atau direktori pada sistem file. Selain itu, NameNode meregulasi akses pengguna terhadap file dan mengatur blok mana yang akan diolah oleh DataNode ~\cite{tomwhite:05:htdg}. NameNode membuat semua keputusan terkait replikasi blok. NameNode secara berkala menerima \textit{heartbeat} dan \textit{block report} dari masing-masing DataNode di \textit{cluster}. \textit{Heartbeat} mengimplikasikan bahwa DataNode berfungsi dengan benar.\\
	
\textit{Slave node} atau dapat disebut DataNode merupakan pekerja dari HDFS \cite{tomwhite:05:htdg}. DataNode bertanggung jawab untuk menjalankan perintah membaca dan menulis untuk file sistem Hadoop. NameNode dapat membuat, menghapus, dan mereplikasi blok ketika diberi instruksi dari \textit{master node}. DataNode menyimpan dan mengambil blok ketika diperintahkan oleh NameNode. Selain itu, DataNode melaporkan daftar blok-blok yang disimpan kepada NameNode secara rutin.\\ 

HDFS dirancang untuk menyimpan file yang berukuran sangat besar di seluruh mesin dalam \textit{cluster} yang besar \cite{tomwhite:05:htdg}. HDFS menyimpan setiap file sebagai blok yang berurutan. Semua blok dalam file kecuali blok terakhir memiliki ukuran yang sama. Bisa dilihat pada Gambar~\ref{fig:hdfshadoop} bahwa blok-blok file direplikasi untuk memiliki \textit{fault tolerance} yang tinggi. Ukuran blok dan banyaknya replika dapat dikonfigurasi untuk setiap file. Faktor replikasi dapat ditentukan pada waktu pembuatan file dan dapat diubah nantinya.\\

Berikut adalah perintah-perintah dasar yang dapat digunakan untuk HDFS~\cite{chucklam:06:hia}:

\begin{itemize}
\item Perintah untuk membuat direktori HDFS untuk penyimpanan file.

\begin{verbatim}
$ hadoop fs -mkdir <dir-path>
\end{verbatim}
 
\item Perintah untuk melihat daftar konten direktori dari \textit{path} yang diberikan.

\begin{verbatim}
$ hadoop fs -ls 
\end{verbatim}

\item Perintah untuk memasukkan file atau direktori lokal kepada file sistem destinasi di dalam HDFS.

\begin{verbatim}
$ hadoop fs -put <localSrc> <dest> 
\end{verbatim}

\end{itemize}


\subsection{MapReduce}~\cite{tomwhite:05:htdg}

MapReduce adalah sebuah model pemrograman untuk memproses data berukuran besar secara terdistribusi dan paralel dalam \textit{cluster} yang terdiri atas banyak komputer. Dalam memproses data, secara garis besar MapReduce dapat dibagi dalam dua proses, yaitu proses \textit{map} dan proses \textit{reduce}. Setiap fase memiliki pasangan \textit{key-value} sebagai \textit{input} dan \textit{output}. Kedua jenis proses ini didistribusikan ke setiap komputer dalam suatu \textit{cluster} dan berjalan secara paralel tanpa saling bergantung satu sama yang lainnya. Proses \textit{map} bertugas untuk mengumpulkan informasi dari potongan-potongan data yang terdistribusi dalam tiap komputer dalam cluster. Hasilnya diserahkan kepada proses \textit{reduce} untuk diproses lebih lanjut. Hasil proses \textit{reduce} merupakan hasil akhir.\\

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.5]{mapreducehadoop}  
    \caption[Arsitektur MapReduce]{Arsitektur MapReduce} 
    \label{fig:mapreducehadoop} 
\end{figure}

Gambaran arsitektur MapReduce dapat dilihat pada Gambar~\ref{fig:mapreducehadoop} yaitu arsitektur MapReduce. Pada arsitektur ini, \textit{master node} disebut JobTracker dan \textit{slave node} disebut TaskTracker. JobTracker adalah jembatan antara pengguna dan fungsi \textit{map} maupun \textit{reduce}. Ketika sebuah pekerjaan \textit{map} atau \textit{reduce} diterima oleh JobTracker, pekerjaan tersebut akan dimasukkan ke dalam antrian. Pekerjaan dalam antrian akan dikerjakan sesuai urutan masuk pekerjaan tersebut. Kemudian, pekerjaan akan ditugaskan kepada TaskTracker oleh JobTracker. TaskTracker akan mengeksekusi pekerjaan yang diberikan oleh JobTracker dan mengembalikan laporan kemajuan kepada JobTracker.   


\begin{figure}[H]
    \centering  
    \includegraphics[scale=1]{mpstep}  
    \caption[Proses MapReduce]{Proses MapReduce} 
    \label{fig:mpstep} 
\end{figure}

Berdasarkan Gambar~\ref{fig:mpstep}, berikut adalah langkah-langkah proses  MapReduce:

\begin{enumerate}
\item \textit{Input} dibagi menjadi \textit{input split} yang berukuran sama. Setiap \textit{input splits} akan dibuatkan \textit{map task}.

\item Pada fase \textit{map}, data pada setiap \textit{split} akan dihitung berapa banyak kemunculan kata tersebut dan dijadikan pasangan <\textit{word}, \textit{frequency}> sebagai \textit{output}.

\item Fase selanjutnya adalah fase \textit{shuffling}. Tahap ini akan mengirim \textit{output} dari fase \textit{map} kepada \textit{reducer}. Hasil dari fase \textit{map} akan dikelompokan berdasarkan \textit{key} dan dibagi di antara \textit{reducer}. Dalam contoh ini, kata-kata yang sama disatukan bersama dengan frekuensi masing-masing.


\item Terakhir adalah fase \textit{reduce} di mana \textit{output} dari \textit{shuffling} akan dikumpulkan. Nilai-nilai dari fase \textit{shuffling} akan digabungkan menjadi sebuah \textit{output}. \textit{Output} akan disimpan pada HDFS.

\end{enumerate}

\subsection{YARN}

Apache YARN (\textit{Yet Another Resource Negotiator}) adalah pengatur sumber daya dari \textit{cluster} Hadoop. YARN bertujuan untuk memisahkan fungsionalitas antara pengaturan sumber daya dan penjadwalan pekerjaan. YARN memiliki dua tipe \textit{daemon} yaitu \textit{Resource Manager} dan \textit{Node Manager}~\cite{tomwhite:05:htdg}.  \textit{Resource Manager} bertugas untuk mengatur sumber daya di seluruh \textit{cluster} dan \textit{Node Manager} yang berjalan pada \textit{node}. \textit{Node Manager} bertugas untuk menjalankan dan memantau \textit{container}~\cite{tomwhite:05:htdg}. \textit{Container} bertugas untuk mengeksekusi proses aplikasi yang spesifik.

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.6]{yarn}  
    \caption[Proses menjalankan aplikasi pada YARN]{Proses menjalankan aplikasi pada YARN} 
    \label{fig:yarn} 
\end{figure}

Berikut adalah Gambar~\ref{fig:yarn} yang menggambarkan langkah-langkah proses ketika menjalankan aplikasi pada YARN. Untuk menjalankan aplikasi pada YARN, \textit{client} akan meminta \textit{Resource Manager} untuk menjalankan proses aplikasi \textit{master} (langkah 1). Kemudian, \textit{Resource Manager} akan mencari \textit{Node Manager} yang bisa menjalankan aplikasi \textit{master} dalam sebuah \textit{container} (langkah 2 dan 3). Ketika aplikasi \textit{master} sudah berjalan, aplikasi \textit{master} bisa melakukan komputasi pada \textit{container} dan mengembalikan hasil kepada \textit{client}. Selain itu, aplikasi \textit{master} dapat juga meminta sumber daya tambahan (langkah 4) dan menggunakan sumber daya tersebut untuk komputasi terdistribusi (langkah 5 dan 6).\\

\section{Spark}~\cite{holdenkarau:07:ls}

Apache Spark adalah sebuah \textit{cluster computing platform} yang dirancang untuk kecepatan dan \textit{general-purpose}. Spark dirancang berdasarkan model MapReduce yang populer untuk memberikan dukungan yang efisien kepada banyak tipe komputasi, termasuk \textit{interactive query} dan \textit{stream processing}. Kecepatan merupakan kunci dalam melakukan eksplorasi data. Rentang waktu dalam eksplorasi dapat dimulai dari beberapa menit sampai beberapa jam. Salah satu fitur utama Spark yang ditawarkan adalah kemampuannya untuk melakukan \textit{in memory computations}. Selain itu, sistem Spark lebih efisien daripada MapReduce dalam menjalankan aplikasi yang rumit pada \textit{disk}.\\


Pada sisi \textit{general-purpose}, Spark dirancang untuk mencakup berbagai beban kerja yang sebelumnya diperlukan sistem terdistribusi terpisah, termasuk aplikasi \textit{batch}, \textit{iterative algorithms}, \textit{interactive query}, dan \textit{streaming}. Dengan mendukung beban kerja tersebut di mesin yang sama, Spark membuat pekerjaan lebih mudah dan murah untuk menggabungkan pemrosesan yang berbeda jenis. Dengan begitu, Spark mengurangi beban dalam merawat \textit{tools} yang terpisah.\\

Spark dirancang untuk memudahkan pengaksesan dengan memberikan API sederhana untuk Python, Java, Scala, dan SQL. Spark dengan mudah berintegrasi dengan tools \textit{Big Data} lainnya, terutama Hadoop. Spark bisa berjalan pada Hadoop \textit{cluster} dan mengakses sumber data Hadoop manapun.\\





\subsection{Komponen Spark}~\cite{holdenkarau:07:ls}

Spark memiliki beberapa komponen yang terintegrasi dengan erat. Sebagai \textit{core}, Spark adalah "mesin komputasi" yang bertanggung jawab untuk penjadwalan, distribusi, dan pemantauan aplikasi yang terdiri atas banyak \textit{task} komputasi tersebar di banyak pekerja, mesin, atau \textit{cluster}. Karena \textit{core engine} dari Spark sangat cepat dan dirancang untuk tujuan umum, Spark menjalankan banyak komponen di level yang lebih tinggi untuk menangani berbagai macam pekerjaan khusus seperti SQL atau \textit{machine learning}. Komponen-komponen ini dirancang untuk saling beroperasi dengan erat. Spark mengizinkan pengguna untuk menggabungkan komponen seperti \textit{library} dalam suatu proyek perangkat lunak.\\

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.6]{sparkcomponent}  
    \caption[Komponen pada Spark]{Komponen pada Spark} 
    \label{fig:sparkcomponent} 
\end{figure}

Bedasarkan Gambar ~\ref{fig:sparkcomponent}, Spark memiliki beberapa komponen sebagai berikut:

\begin{itemize}

\item Spark Core:
Spark Core berisi fungsi-fungsi dasar Spark, termasuk komponen untuk tugas penjadwalan, manajemen memori, pemulihan kesalahan, berinteraksi dengan sistem penyimpanan,
dan banyak lagi. Spark Core memiliki banyak API \textit{resilient distributed datasets}(RDD), yang merupakan abstraksi pemrograman utama Spark. RDD mewakili suatu koleksi objek-objek yang didistribusikan di banyak node komputasi yang dapat dimanipulasi
secara paralel. Spark Core menyediakan banyak API untuk membangun dan memanipulasi RDD.

\item Spark SQL: Spark SQL adalah sebuah modul untuk mengerjakan data yang terstruktur. Modul ini memungkinkan melakukan \textit{query} pada data terstruktur melalui SQL serta varian Apache Hive dari SQL yang disebut Hive Query Language (HQL) dan mendukung banyak sumber data, termasuk tabel Hive, Parket, dan JSON. Selain menyediakan antarmuka SQL untuk Spark, Spark SQL memungkinkan \textit{developer} untuk memadukan kueri SQL dengan fungsi-fungsi pada RDD. 

\item Spark Streaming: Spark Streaming adalah komponen Spark yang memungkinkan pemrosesan data dari \textit{live streaming}. Contoh \textit{data stream} termasuk file log yang dihasilkan oleh server web produksi, atau antrian pesan yang berisi pembaruan status yang diunggah oleh pengguna layanan web. Spark Streaming menyediakan API yang mirip dengan  Spark Core's RDD API untuk memanipulasi aliran data. Hal ini membuat \textit{developer} mudah mempelajari proyek dan berpindah antar aplikasi yang memanipulasi data yang disimpan dalam memori, pada \textit{disk}, atau yang tiba dalam \textit{real time}. Di balik API-nya, Spark Streaming dirancang untuk menyediakan tingkat toleransi kesalahan, \textit{throughput}, dan skalabilitas yang sama seperti Spark Core.

\item MLlib: Spark hadir dengan \textit{library} yang berisi fungsi pembelajaran mesin (ML) secara umum, \textit{library} ini disebut MLlib. MLlib menyediakan beberapa jenis algoritma pembelajaran mesin, termasuk klasifikasi, regresi, pengelompokan, dan penyaringan kolaboratif, serta pendukung
fungsionalitas seperti \textit{model evaluation} dan \textit{data import}. MLlib juga menyediakan
beberapa \textit{lower-level} ML \textit{primitives}, termasuk \textit{generic gradient descent optimization
algorithm}.

\item GraphX: GraphX adalah sebuah \textit{library} untuk memanipulasi grafik dan melakukan \textit{graph-parallel computations}. Seperti Spark Streaming dan Spark SQL, GraphX memperluas API Spark RDD, memungkinkan pengguna untuk membuat \textit{directed graph} dengan \textit{arbitrary properties} yang melekat pada setiap \textit{vertex} dan \textit{edge}. GraphX juga menyediakan berbagai operator untuk memanipulasi grafik dan memiliki \textit{library} yang penuh dengan \textit{graph algorithms} yang umum seperti PageRank dan \textit{triangle counting}.

\item Cluster Managers: Spark dirancang untuk dapat ditambah secara efisien dari satu hingga ribuan node komputasi. Untuk mencapai hal ini dan memaksimalkan fleksibilitas, Spark dapat menjalankan lebih dari satu variasi manajer \textit{cluster} seperti Hadoop YARN, Apache Mesos, \textit{simple
cluster manager} pada diri Spark sendiri yang disebut \textit{Standalone Scheduler}.\\

\end{itemize}

\subsection{Tiga Cara Membangun Spark di Atas Hadoop}

\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.6]{sparkins}  
    \caption[Macam-macam cara instalasi Spark]{Macam-macam cara instalasi Spark} 
    \label{fig:sparkins} 
\end{figure}

Berdasarkan Gambar~\ref{fig:sparkins} terdapat tiga cara untuk menginstal Spark. Ketiga cara tersebut antara lain:

\begin{itemize}

\item \textit{Standalone}: Spark \textit{standalone} berarti Spark menempati tempat di atas HDFS (Hadoop Distributed File System) dan ruang dialokasikan untuk HDFS, secara eksplisit. Spark dan MapReduce akan berjalan berdampingan untuk mencakup semua pekerjaan di \textit{cluster}.

\item Hadoop YARN: Spark berjalan pada YARN tanpa perlu pra-instalasi atau akses root. Cara ini membantu mengintegrasikan Spark ke dalam ekosistem Hadoop. Cara ini memungkinkan komponen lain untuk berjalan di atas tumpukan.

\item Spark pada MapReduce: Spark pada MapReduce digunakan untuk menjalankan pekerjaan-pekerjaan pada spark selain untuk \textit{standalone deployment}. Pengguna dapat memulai Spark dan menggunakan \textit{Spark Shell} tanpa akses administratif.\\

\end{itemize}









\subsection{Arsitektur Spark}

Spark menggunakan arsitektur \textit{master} dan \textit{slave}. Sebuah Spark \textit{cluster} memiliki satu \textit{master} dan banyak \textit{slave} atau bisa disebut sebagai \textit{worker}. Spark memiliki beberapa komponen penting dalam arsitekturnya seperti \textit{Driver Program}, \textit{Spark Context}, \textit{Cluster Manager}. Gambar~\ref{fig:sparkarc} menggambarkan komponen-komponen arsitektur Spark.


\begin{figure}[H]
    \centering  
    \includegraphics[scale=0.5]{sparkarc}  
    \caption[Arsitektur Spark]{Arsitektur Spark} 
    \label{fig:sparkarc} 
\end{figure}

Berikut adalah penjelasasn dari komponen-komponen Gambar Gambar~\ref{fig:sparkarc}:

\begin{itemize} 

\item \textbf{\textit{Driver Program}}\\
\textit{Driver program} yang berjalan pada \textit{master node} bertugas menjalankan fungsi main() dari aplikasi dan tempat di mana \textit{Spark Context} dibuat. Kode program akan diterjemahkan menjadi \textit{tasks} dan dijadwalkan kepada \textit{executors} untuk dikerjakan. \textit{Driver program} akan berkomunikasi dengan \textit{cluster manager} untuk mengatur sumber daya pada \textit{cluster}.\\

\item \textbf{\textit{Spark Context}}\\
\textit{Spark Context} menghubungkan pengguna dengan \textit{cluster}. \textit{Spark Context} dapat terhubung dengan beberapa \textit{cluster manager} seperti YARN, MESOS, dan \textit{Spark standalone cluster manager}. \textit{Spark Context} dapat digunakan untuk membuat \textit{Resilient Distributed Datasets} (RDD), \textit{accumulators}, dan \textit{broadcast variable}. 

\item \textbf{\textit{Cluster Manager}}\\
\textit{Cluster Manager} berfungsi mengatur sumber daya pada sebuah \textit{cluster}. Spark dapat berjalan pada berbagai macam \textit{cluster manager} seperti Apache Mesos, Hadoop YARN, dan \textit{Spark's stand alone}. \textit{Cluster manager} akan berusaha mendapatkan sumber daya pada \textit{cluster} dan mealokasikannya kepada Spark \textit{job} yang sedang berjalan.

\item \textbf{\textit{Executors}}\\
\textit{Executors} adalah proses-proses yang berjalan pada \textit{worker node} dan bertanggung jawab untuk mengerjakan \textit{tasks} yang diberikan. \textit{Executors} dibuat ketika aplikasi dijalankan dan akan tetap ada selama aplikasi masih berjalan.

\item \textbf{\textit{Tasks}}\\
\textit{Task} adalah sebuah satuan kerja pada Spark. \textit{Task} berisi perintah-perintah. Perintah tersebut merupakan fungsi yang diserialisasi. \textit{Task} akan dikirimkan oleh \textit{driver program} kepada \textit{executor}. Kemudian, \textit{executor} akan medeserialisai perintah tersebut dan mengerjakannya. Pada umumnya \textit{task} akan dibuat untuk setiap partisi. Partisi merupakan potongan data yang terdistribusi pada \textit{cluster}.


\end{itemize}







\subsection{\textit{Resilient Distributed Datasets} (RDD)}

\textit{Resilient Distributed Datasets} (RDD) adalah struktur data dasar pada Spark yang berisi koleksi benda-benda yang didistribusikan secara permanen. Setiap dataset dalam RDD dibagi menjadi beberapa partisi yang dapat dikomputasi pada node yang berbeda pada \textit{cluster}~\cite{holdenkarau:07:ls}. RDD dapat berisi jenis objek Python, Java, atau Scala, termasuk kelas yang ditentukan pengguna. Spark memanfaatkan konsep RDD untuk mencapai operasi MapReduce yang lebih cepat dan efisien~\cite{holdenkarau:07:ls}.

Secara umum, RDD merupakan kumpulan \textit{read-only, partitioned collection} dari \textit{records}. RDD dapat dibuat melalui operasi deterministik dari data pada penyimpanan yang stabil atau RDD lainnya~\cite{holdenkarau:07:ls}. Elemen pada RDD memiliki sifat \textit{fault tolerance} dan dapat dioperasikan secara paralel.


\textit{Data sharing} pada MapReduce lebih lambat dibanding RDD  karena replikasi, serialisasi, dan disk IO. Sebagian besar aplikasi Hadoop menghabiskan lebih dari 90 persen waktunya untuk melakukan operasi \textit{read-write} kepada HDFS.

Untuk menangani masalah tersebut, dibangun \textit{framework} khusus yang disebut Apache Spark. Ide utama dari Spark adalah RDD, Spark juga mendukung \textit{in-memory computation}. Spark menyimpan status memori sebagai objek di seluruh pekerjaan dan objek dapat dibagi di antara \textit{jobs}. \textit{Data sharing} dalam memori lebih cepat 10 hingga 100 kali lipat dibanding \textit{network} atau \textit{disk}.

Berikut adalah sifat-sifat dari RDD~\cite{holdenkarau:07:ls}:
\begin{itemize}

\item \textit{In Memory}: Data pada RDD disimpan pada memori sebesar mungkin dan selama mungkin.

\item \textit{Partitioned}: \textit{records} dipartisi dan didistribusikan kepada \textit{node}-\textit{node} di dalam \textit{cluster}.

\item \textit{Typed}: RDD memiliki tipe data seperti RDD[Long], RDD[String] dan tipe data lainnya.

\item \textit{Lazy evaluation}: Data di dalam RDD tidak akan tersedia atau berubah sampai sebuah perintah \textit{action} telah dieksekusi.

\item \textit{Immutable}: RDD yang telah dibuat tidak dapat berubah. Meskipun demikian, RDD dapat ditransformasi menjadi sebuah RDD baru dengan melakukan perintah \textit{transformation} pada RDD.

\item \textit{Parallel}: RDD dapat dioperasikan secara paralel.

\item \textit{Cacheable}: Pengguna dapat memilih RDD mana yang akan dipakai kembali dan memilih tempat penyimpanannya, yaitu memori atau \textit{disk}. Dengan begitu, data dapat diakses lebih cepat untuk permintaan selanjutnya.\\

\end{itemize}

Terdapat dua cara untuk membuat sebuah RDD. Cara pertama adalah dengan memuat dataset eksternal, sedangkan cara alternatif adalah dengan mendistribusikan sebuah koleksi objek seperti \textit{list} atau \textit{set}~\cite{holdenkarau:07:ls}. Terdapat dua tipe operasi yang dapat dilakukan RDD, yaitu \textit{transformations} dan \textit{actions}. \textit{Transformations} membuat RDD baru dari RDD sebelumnya~\cite{holdenkarau:07:ls}. Berbeda dengan \textit{transformations}, \textit{actions} mengembalikan nilai hasil komputasi berdasarkan RDD~\cite{holdenkarau:07:ls}. Hasil dari \textit{actions} akan dikembalikan kepada \textit{driver program} atau disimpan pada penyimpanan eksternal seperti HDFS.\\

Berikut adalah contoh pembuatan RDD dari sumber eksternal dan koleksi objek:
\begin{itemize}

\item
\begin{verbatim}
val lines = sc.textFile("/path/to/README.md") \\ sumber eksternal
\end{verbatim}

\item
\begin{verbatim}
val lines = sc.parallelize(["a", "b", "c", "d", "e"]) \\ array 
\end{verbatim}

\end{itemize}


\textit{Transformations} pada RDD adalah sebuah operasi yang menerima RDD sebagai masukan dan mengembalikan satu atau lebih RDD baru. RDD masukan tidak berubah karena sifat RDD adalah \textit{immutable} yang berarti tidak bisa diubah ketika dibuat. \textit{Transformations} bersifat \textit{lazy} dan tidak langsung dieksekusi, Spark akan mencatat \textit{transfomartion} apa saja yang dilakukan pada RDD sejak awal. \textit{Transformations} akan dieksekusi ketika sebuah \textit{actions} dipanggil.\\

Berikut adalah contoh \textit{filter} \textit{transformation} di Scala. \textit{Filter} digunakan untuk menyaring elemen-elemen yang sesuai dengan kriteria yang ditentukan. Pada kasus ini, filter akan mengambil baris-baris yang memiliki kata \textit{error}.
\begin{verbatim}
val inputRDD = sc.textFile("log.txt") 
val errorsRDD = inputRDD.filter(line => line.contains("error"))
\end{verbatim}

Tabel~\ref{tab:trans} berisi daftar \textit{transformations} yang umum pada Spark:

\begin{table}[H] 
	\centering 
	\caption{Tabel transformations}
	\label{tab:trans}
	\begin{tabular}{p{6cm}p{9cm}}
		\toprule[1.5pt]
\hline
		 \textbf{\textit{Transformations}} & Penjelasan \\
\hline
\midrule

\hline
\textbf{map}(func) & Mengembalikan RDD baru yang dibentuk dengan melewatkan setiap elemen melalui fungsi func. \\
\hline

\textbf{mapPartitions}(func) & Mengembalikan RDD baru yang dibentuk dengan melewatkan setiap partisi melalui fungsi func.\\

\hline

\textbf{filter}(func) & Mengembalikan RDD baru yang dibentuk dengan memilih elemen-elemen yang mengembalikan nilai \textit{true} dari fungsi func. \\
\hline

\textbf{flatMap}(func) & Mirip dengan \textit{map}, tetapi setiap elemen dapat dipetakan menjadi nol atau lebih elemen sebagai keluaran. \\
\hline


\textbf{union}(otherDataset) & Mengembalikan RDD baru yang mengandung elemen dari kedua sumber.\\

\hline
\textbf{intersection}(otherDataset) & Mengembalikan RDD baru yang berisi potongan elemen dari sumber dan sumber lainya.\\ 

\hline
\textbf{distinct}([numPartitions]) & Mengembalikan RDD baru yang mengandung elemen yang unik dari sumber.\\

\hline
\textbf{groupByKey}([numPartitions]) & Mengembalikan RDD baru bertipe \textit{pairs}  (K, Iterable<V>) dari sumber RDD bertipe (K, V).\\


\hline
\textbf{groupByKey}(func,[numPartitions]) & Mengembalikan RDD baru berupa \textit{pairs} (K, V) yang sudah diagregasi bedasarkan \textit{key} dan fungsi \textit{reduce} yang diberikan.\\

\hline
\textbf{sortByKey}([ascending], [numPartitions]) & Mengembalikan RDD baru berupa \textit{pairs}  (K, V) yang terurut secara menaik atau menurun badsarkan parameter boolean yang diberikan.\\

\hline
\textbf{join}(otherDataset, [numPartitions]) & Mengembalikan gabungan RDD berupa \textit{pairs}  (K, V) dan (K, W) menjadi \textit{pairs} (K, (V,W)).\\

\hline


\bottomrule
		
	\end{tabular} 
\end{table}

Berikut adalah contoh operasi \textit{action} pada RDD. Pada contoh ini, fungsi \textit{reduceByKey} digunakan untuk menghitung jumlah kata yang ada.

\begin{verbatim}
    val lines = sc.textFile("data.txt") 
    val pairs = lines.map(s => (s, 1))
    val counts = pairs.reduceByKey((a, b) => a + b)
\end{verbatim}

\textit{Actions} merupakan operasi yang mengembalikan sebuah nilai kepada \textit{driver program} atau tempat penyimpanan eksternal. Untuk mengembalikan sebuah nilai, dapat digunakan fungsi-fungsi seperti take(), count(), collect(), dan \textit{actions} lainya. Operasi take() digunakan untuk mengambil sebagian kecil elemen pada RDD. Ketika menggunakan collect(), memori pada satu komputer harus cukup untuk menampung seluruh \textit{data set} ~\cite{holdenkarau:07:ls}. Operasi tersebut sebaiknya digunakan pada \textit{data set} yang berukuran kecil. \textit{Data set} yang berukuran besar dapat disimpan pada tempat penyimpanan eksternal. Setiap kali sebuah \textit{actions} dipanggil, seluruh RDD akan dikomputasi dari akarnya. Untuk mencapai efisiensi yang lebih tinggi, dapat dilakukan \textit{persist} terhadap \textit{intermediate results}. \\

Berikut adalah Tabel~\ref{tab:actions} berisi daftar \textit{actions} yang umum pada Spark:


\begin{table}[H] 
	\centering 
	\caption{Tabel Actions}
	\label{tab:actions}
	\begin{tabular}{p{6cm}p{9cm}}
		\toprule[1.5pt]
\hline
		 \textbf{\textit{Actions}} & Penjelasan \\
\hline
\midrule

\hline
\textbf{reduce}(func) & Mengagregasikan seluruh elemen pada RDD menggunakan fungsi yang diberikan pada \textit{parameter}. \\

\hline
\textbf{collect}() & Mengembalikan seluruh \textit{data set} sebagai \textit{array} kepada \textit{driver program}. \\

\hline
\textbf{count}() & Mengembalikan jumlah elemen pada RDD. \\

\hline
\textbf{first}() & Mengembalikan elemen pertama pada RDD. \\

\hline
\textbf{take}(n) & Mengembalikan sebuah \textit{array} dengan n jumlah elemen pertama dari RDD.\\

\hline
\textbf{takeOrdered}(n, [ordering]) & Mengembalikan sebuah \textit{array} dengan n jumlah elemen pertama dari RDD secara terurut. \\

\hline
\textbf{saveAsTextFile}(path) & Menyimpan \textit{dataset} sebagai \textit{text file} pada direktori yang ditentukan. \\

\hline
\textbf{saveAsSequenceFile}(path) & Menyimpan RDD sebagai Hadoop SequenceFile pada direktori yang ditentukan.\\

\hline
\textbf{saveAsObjectFile}(path) & Menyimpan RDD sebagai format yang sederhana menggunakan Java Serialization pada direktori yang ditentukan.\\

\hline
\textbf{countByKey}() & Menjumlahkan \textit{pairs} (K, V) berdasarkan \textit{key} dan mengembalikan sebuah \textit{pairs} berisi (K, int). \\

\hline
\textbf{foreach}(func) & Memproses setiap elemen pada RDD menggunakan fungsi func yang diberikan. \\

\hline


\bottomrule
	\end{tabular} 
\end{table}



\section{Scala}


Scala adalah sebuah bahasa pemrograman yang diciptakan oleh Martin Odersky, yaitu seorang Profesor di Ecole Polytechnique Federale de Lausanne, sebuah kampus di Lausanne, Swiss. Kata Scala sendiri merupakan singakatan dari "Scalable Language". Karena Scala berjalan di atas \textit{Java Virtual Machine} (JVM), Scala memiliki performa yang relatif cepat dan juga memungkinkan untuk menggabungkan kode di Scala dengan di Java. library, framework dan tool yang ada di Java dapat gunakan pada Scala. Scala menggabungkan konsep \textit{Object Oriented Programming} (OOP) yang dikenal di Java dengan konsep \textit{Functional Programming} (FP). Adanya konsep FP inilah yang menjadikan Scala sangat ekspresif, nyaman dan menyenangkan untuk digunakan. \\

Perintah scalac digunakan untuk mengkompilasi program Scala dan akan menghasilkan beberapa file kelas di direktori saat ini. Salah satunya akan disebut file .class. Ini adalah \textit{bytecode} yang akan berjalan di JVM dengan menggunakan perintah scala. \\

\subsection{\textit{Expressions}}

\textit{Expressions} adalah pernyataan atau argumen yang dapat dikomputasi.

\begin{verbatim}
1 + 1
2 + 2 
\end{verbatim}

\textit{Expressions} dapat dikembalikan dengan perintah \textit{println}.

\begin{verbatim}
println(1)
println(100) // 100
println(1 + 1) // 2
println("Hi!") // Hi!
\end{verbatim}


\textit{Expressions} atau pernyataan seperti di atas dapat disimpan dalam sebuah \textit{variable}. Terdapat dua jenis \textit{variable} di Scala yaitu \textit{val} dan \textit{var}. Setelah \textit{val} diinisialisasi, \textit{val} tidak dapat diisi kembali yang berarti nilai dari \textit{val} tidak dapat diubah.

\begin{verbatim}
val x = 2 + 5
val x = 10 //tidak akan di-compile 
val y = 7
val coba:Int = 200 
\end{verbatim}

\textit{variable} mirip dengan value, tetapi nilai \textit{variable} dapat diisi kembali.

\begin{verbatim}
var x = 2 + 2 
x = 4 
println(x) // 4 
x = 7 
println(x) // 7 
\end{verbatim}

Secara eksplisit, \textit{developer} dapat menyatakan tipe dari sebuah \textit{var} atau \textit{val} dengan cara:

\begin{verbatim}
var x: Int = 1 + 1 // Int merupakan tipe dari variable x
val y: Long = 987654321 // Long merupakan tipe dari variable y
val z: Char = 'a' // Char merupakan tipe dari variable z
\end{verbatim}

\subsection{\textit{Blocks}}

\textit{Block} digunakan untuk menggabungkan \textit{expressions}. Berikut adalah contoh \textit{blok}:

\begin{verbatim}
println({
    val x = 1 + 1
    x + 1
}) // 3 
\end{verbatim}

\subsection{\textit{Loop} dan \textit{Conditional}}

\textit{loop} merupakan struktur pengulangan yang memungkinkan menulis suatu \textit{loop} yang perlu dieksekusi sekian kali secara efisien. Terdapat berbagai bentuk \textit{loop} dalam Scala yang dijelaskan di bawah ini: 

\begin{verbatim}
for( var x <- Range ){
   statement(s);
}

var x = 0
while (x < 10) {
    println(x)
    x += 1
}
\end{verbatim}

\textit{COnditional} atau percabangan adalah pengujian sebuah kondisi. Jika kondisi yang diuji tersebut terpenuhi, maka program akan menjalankan pernyataan-pernyataan tertentu. Jika kondisi yang diuji salah, program akan menjalankan pernyataan yang lain. Berikut adalah contoh percabangan dalam bahasa Scala:

\begin{verbatim}
if( x < 20 ){
    println("This is if statement");
}

if( x < 20 ){
    if( x< 5) {
        println("smallest");
    }
}

if( x < 10 ){
    println("This is bigger");
} else { 
    println("This is smaller");
}

if( x == 1 ){
    println("1");
} else if (x == 2){ 
    println("2");
}
\end{verbatim}


\subsection{Functions}

\textit{Functions} adalah \textit{expression} yang mempunyai atau menerima parameter. Sebuah \textit{function} yang tidak memiliki nama disebut \textit{anonymous function}. Berikut adalah contoh \textit{anonymous function} dan \textit{function} biasa. Sebuah \textit{function} dapat memiliki lebih dari satu parameter.

\begin{verbatim}
(x: Int) => x + 1 // Anonymous function 

val addOne = (x: Int) => x + 1 // function biasa 
println(addOne(2)) // 3 

val add = (x: Int, y: Int) => x + y 
println(add(1, 2)) // 3 
\end{verbatim}

Pada sisi sebelah kiri tanda "=>"" adalah parameter-parameter sebuah \textit{function}, sementara pada sisi  sebelah kanan merupakan ekspresi-ekspresi yang melibatkan parameter tersebut. \\


\subsection{Methods}

\textit{Method} sangat mirip dengan \textit{function}, tetapi \textit{method} memiliki beberapa perbedaan. Method harus didefinisikan dengan kata kunci \textit{def}, diikuti dengan nama \textit{method}, parameter-parameter dari \textit{method} tersebut, tipe kembalian \textit{method}, dan isi dari \textit{method} tersebut. 

\begin{verbatim}
def add(x: Int, y: Int): Int = x + y 
println(add(1, 2)) // 3 
\end{verbatim}

\textit{Method} dapat mempunyai lebih dari satu parameter. 

\begin{verbatim}
def addThenMultiply(x: Int, y: Int)(multiplier: Int): Int = (x + y) * multiplier 
println(addThenMultiply(1, 2)(3)) // 9 
\end{verbatim}


\textit{Method} dapat tidak memiliki parameter.

\begin{verbatim}
def name: String = System.getProperty("user.name")
println("Hello, " + name + "!")
\end{verbatim}

\textit{Method} berbeda dengan \textit{functions} dapat memiliki \textit{multi-line expressions}

\begin{verbatim}
def getSquareString(input: Double): String = {
    val square = input * input
    square.toString
}
\end{verbatim}

\textit{Expression} terakhir dari \textit{method} menjadi nilai yang akan dikembalikan. Scala mempunyai \textit{keyword} return, tetapi sangat jarang digunakan.

\subsection{Class dan Object}

\textit{Class} pada Scala didefinisikan dengan kata kunci \textit{class} yang diikuti dengan namanya dan terakhir adalah \textit{constructor} parameter.

\begin{verbatim}

class Greeter(prefix: String, suffix: String) {
    def greet(name: String): Unit = {
        println(prefix + name + suffix)
    }
}

\end{verbatim}



Berikut adalah cara mendeklarasi sebuah objek pada Scala
\begin{verbatim}
val greeter = new Greeter("Hello, ", "!")
greeter.greet("Scala developer")
\end{verbatim}

Objek dapat dianggap sebagai suatu instansi tunggal pada kelas itu sendiri. Kata kunci \textit{object} dapat digunakan untuk mendefinisikan sebuah objek.

\begin{verbatim}
object IdFactory {
    private var counter = 0
Main method
  	def create(): Int = {
        counter += 1
        counter
    }
}

val newId: Int = IdFactory.create()
println(newId) // 1
val newerId: Int = IdFactory.create()
println(newerId) // 2

\end{verbatim}

\textit{Main method} adalah pintu masuk dari sebuah program. JVM membutuhkan sebuah \textit{main method} yang dinamakan \textit{main} dan menerima satu \textit{argument}, yaitu sebuah \textit{array} bertipe \textit{string}. Menggunakan \textit{object}, \textit{developer} dapat mendefinisikan sebuah \textit{main method} seperti berikut: 

\begin{verbatim}
object Main {
  	def main(args: Array[String]): Unit = {
        println("Hello, Scala developer!")
    }
}
\end{verbatim}

\subsection{Higher Order Function}

Pada bahasa Scala, terdapat sebuah fungsi yang disebut sebagai \textit{Higher Order Function}. \textit{higher order function} merupakan sebuah fungsi yang menerima fungsi lainya sebagai \textit{parameter} dan mengembalikan sebuah fungsi sebagai hasilnya. Berikut adalah contoh-contoh \textit{higher order function}:

\begin{verbatim}

val salaries = Seq(20000, 70000, 40000) 
val doubleSalary = (x: Int) => x * 2
val newSalaries = salaries.map(doubleSalary) // List(40000, 140000, 80000)

\end{verbatim}

Kode program dapat dipersingkat dengan menggunakan fungsi \textit{anonymous} dan langsung dimasukkan pada \textit{parameter}.

\begin{verbatim}
val salaries = Seq(20000, 70000, 40000)
val newSalaries = salaries.map(x => x * 2) // List(40000, 140000, 80000)
\end{verbatim}

\textit{Developer} juga dapat memasukkan \textit{method} pada \textit{parameter} \textit{higher order function}, \textit{compiler} Scala akan mengubah sebuah \textit{method} menjadi fungsi.


\begin{verbatim}
case class WeeklyWeatherForecast(temperatures: Seq[Double]) {

    private def convertCtoF(temp: Double) = temp * 1.8 + 32 

    def forecastInFahrenheit: Seq[Double] = temperatures.map(convertCtoF) 
}
\end{verbatim}


Salah satu alasan untuk menggunakan \textit{higher order function} adalah untuk mengurangi kode yang berlebihan. Misalkan terdapat beberapa metode yang dapat menaikkan gaji seseorang dengan berbagai faktor. Tanpa membuat \textit{higher order function}, kode akan terlihat seperti berikut:

\begin{verbatim}

object SalaryRaiser {

    def smallPromotion(salaries: List[Double]): List[Double] =
        salaries.map(salary => salary * 1.1)

  	def greatPromotion(salaries: List[Double]): List[Double] =
        salaries.map(salary => salary * math.log(salary))

  	def hugePromotion(salaries: List[Double]): List[Double] =
        salaries.map(salary => salary * salary)
}
\end{verbatim}

Perhatikan bahwa masing-masing dari ketiga \textit{method} hanya berbeda pada faktor perkalian. Untuk menyederhanakan kode tersebut, \textit{developer} dapat mengeluarkan kode yang redundan menjadi \textit{higher order function} seperti:

\begin{verbatim}
object SalaryRaiser {

   private def promotion(salaries: List[Double], promoF: Double => Double): List[Double] =
        salaries.map(promotionFunction)  

  	def smallPromotion(salaries: List[Double]): List[Double] =
        promotion(salaries, salary => salary * 1.1)

  	def bigPromotion(salaries: List[Double]): List[Double] =
        promotion(salaries, salary => salary * math.log(salary))

  	def hugePromotion(salaries: List[Double]): List[Double] =
        promotion(salaries, salary => salary * salary)
}
\end{verbatim}





