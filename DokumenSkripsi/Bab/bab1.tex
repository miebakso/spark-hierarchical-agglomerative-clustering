%versi 2 (8-10-2016) 
\chapter{Pendahuluan}
\label{chap:intro}
   
\section{Latar Belakang}
\label{sec:label}

{\it Big data} adalah sebuah istilah yang menggambarkan volume data yang besar, baik data yang terstruktur maupun data yang tidak terstruktur. Data-data tersebut memiliki potensi untuk digali menjadi informasi yang penting. Dalam bidang {\it big data} ada  beberapa tantangan seperti volume data yang besar, kecepatan aliran data yang masuk, dan variasi data dengan format yang berbeda. Tantangan tersebut membuat aplikasi pemrosesan data tradisional  tidak bisa memproses dan menganalisis {\it big data}. Muncul teknologi-teknologi seperti Hadoop dan Spark yang dirancang khusus untuk menangani {\it big data}. 

{\it Big data} akan lebih mudah dianalisis dan diterapkan teknik-teknik {\it data-mining} ketika volume {\it big data} tersebut telah direduksi. Reduksi data adalah proses mengecilkan ukuran data dengan mengambil rangkuman dari sekelompok data. {\it Big data} akan dipecah menjadi beberapa bagian. Dari bagian tersebut akan diolah menggunakan algoritma {\it Hierarchical Agglomerative Clustering} untuk menghasilkan \textit{cluster-cluster}. Dari \textit{cluster-cluster} yang dihasilkan, akan diambil rangkuman yang bisa menggambarkan karakterisik setiap \textit{cluster}. Rangkuman yang menggambarkan karaketristik \textit{cluster} disebut sebagai pola. Pola mengandung jumlah total objek, nilai minimum, maksimum, nilai rata-rata, dan standar-deviasi dari setiap atribut pada \textit{cluster}. Nilai-nilai tersebut diambil karena dapat menggambarkan karakteristik \textit{cluster}. Dengan mereduksi data, kita bisa menghemat biaya pengiriman data, {\it disk space}, dan jumlah data yang diproses. Hasil dari reduksi {\it big data} harus bisa mewakili data yang belum direduksi secara akurat. 

Algoritma {\it Hierarchical Agglomerative Clustering} membangun hirarki dari sekelompok data. Setiap objek akan ditempatkan kepada \textit{cluster}-nya  tersendiri pada awalnya. Kemudian, \textit{cluster} terdekat akan digabung menjadi satu \textit{cluster}. Penggabungan \textit{cluster} terdekat akan diulang sampai hanya satu \textit{cluster} yang tersisa. Proses pembangunan hierarki dapat digambarkan dengan \textit{dendrogram}. \textit{Dendrogram} akan dipotong untuk menghasilkan \textit{cluster-cluster}. Dari setiap \textit{cluster} hasil perpotongan, akan dicari polanya dan disimpan sebagai hasil akhir. 

Untuk mempercepat proses reduksi, pekerjaan dapat dipecah dan dikerjakan secara paralel dengan bantuan Hadoop. Dengan bantuan Hadoop, proses reduksi data akan lebih cepat. Implementasi algoritma {\it Hierarchical Agglomerative Clustering} sudah dilakukan pada Hadoop~\cite{veronica:02:bdhca}. Hasil dari penelitian tersebut membuktikan bahwa algoritma {\it Hierarchical Agglomerative Clustering} sudah dapat mereduksi data dengan menyimpan pola hasil reduksi. Tetapi ada beberapa limitasi yang dimiliki Hadoop. 

Walau Hadoop dapat melakukan proses reduksi secara paralel, waktu yang dibutuhkan Hadoop untuk melakukan reduksi data masih terlalu lambat. Hadoop banyak melakukan proses penulisan dan pembacaan kepada disk. Dari satu tahap ke tahap lainya, Hadoop akan menulis dan membaca hasil sementara kepada disk. Hadoop perlu digantikan dengan Spark untuk mencepat proses reduksi.

Spark adalah {\it distributed cluster-computing framework} yang bisa menggantikan MapReduce beserta kekurangannya. {\it In-memory processing} pada Spark dapat mengalahkan kecepatan pemrosesan pada Hadoop MapReduce. Karena data disimpan pada RAM, kecepatan pemrosesan akan jauh lebih cepat. Spark membaca data yang akan direduksi dari RAM. Pembacaan data dari RAM akan lebih cepat dibanding disk.

Pada skripsi ini, dibangun sebuah perangkat lunak yang dapat mereduksi {\it big data}. Perangkat lunak tersebut akan dibangun menggunakan \textit{framework} terdistribusi Spark dan mengimplementasikan algoritma {\it Hierarchical Agglomerative Clustering} yang khusus dirancang untuk lingkungan Spark. Perangkat lunak dapat menampilkan hasil reduksi dalam format tabel. Dengan menggunakan Spark, waktu proses reduksi data menjadi lebih cepat dibanding MapReduce.




\section{Rumusan Masalah}
\label{sec:rumusan}
Berdasarkan latar belakang di atas, dapat dibentuk rumusan masalah sebagai berikut:
\begin{enumerate}

\item Bagaimana cara kerja algoritma {\it Hierarchical Agglomerative Clustering} berbasis MapReduce untuk mereduksi {\it big data}?

\item Bagaimana cara mengkustomisasi dan mengimplementasikan algoritma {\it Agglomerative Clustering} pada sistem tersebar Spark?

\item Bagaimana mengukur performa hasil dari implementasi algoritma {\it Agglomerative Clustering} pada sistem tersebar Spark?

\end{enumerate}



\section{Tujuan}
\label{sec:tujuan}
Berdasarkan rumusan masalah di atas, tujuan dari penelitian adalah sebagai berikut:
\begin{enumerate}

\item Mempelajari cara kerja algoritma  {\it Hierarchical Agglomerative Clustering} berbasis MapReduce untuk mereduksi {\it big data}.

\item Mengkustomisasi dan mengimplementasikan algoritma {\it Hierarchical Agglomerative Clustering} pada lingkungan Spark.

\item Melakukan eksperimen pada Spark dan Hadoop untuk membandingkan waktu eksekusi kedua perangkat lunak.

\end{enumerate}



\section{Batasan Masalah}
\label{sec:batasan}
Batasan masalah pada skripsi ini adalah sebagai berikut:
\begin{enumerate}

\item Pola yang dikomputasi hanya sebatas jumlah objek, nilai minimum, maksimum, rata-rata dan standar deviasi.

\item Perangkat lunak yang dibuat hanya dapat menerima masukan yang sempurna.


\end{enumerate}


\section{Metodologi}
\label{sec:metlit}
Metodologi yang digunakan dalam pembuatan skripsi ini adalah:
\begin{enumerate}

\item Melakukan studi literatur Hadoop hanya mempelajari konsep dasar dari Hadoop dan \textit{Hadoop Distributed File System} (HDFS).

\item Melakukan studi literatur tentang konsep Apache Spark.

\item Melakukan studi literatur bahasa pemrograman Scala.

\item Melakukan studi literatur tentang algoritma {\it Hierarchical Agglomerative Clustering}.

\item Melakukan instalasi dan konfigurasi Apache Spark.

\item Melakukan eksperimen dengan bahasa pemrograman Scala.

\item Melakukan eksperimen dengan Spark RDD.

\item Melakukan kustomisasi algoritma {\it Hierarchical Agglomerative Clustering} untuk Spark.

\item Mencari dan mengumpulkan data uji coba yang bervolume besar.

\item Merancang dan mengimplementasikan perangkat lunak.

\item Melakukan eksperimen terhadap perangkat lunak dan menganalisis hasil eksperimen.

\item Menulis dokumen skripsi.




\end{enumerate}

\section{Sistematika Pembahasan}
\label{sec:sispem}

Laporan penelitian tersusun ke dalam enam bab secara sistematis sebagai berikut:

\begin{itemize}

\item Bab 1 Pendahuluan\\
Berisi latar belakang, rumusan masalah, tujuan, batasan masalah, metodologi penelitian, dan
sistematika pembahasan.

\item Bab 2 Dasar Teori\\
Berisi dasar teori tentang \textit{big data}, \textit{Hierarchical Agglomerative Clustering}, Hadoop, Spark, dan Scala.

\item Bab 3 Studi dan Eksplorasi Apache Spark\\
Berisi percobaan-percobaan yang dilakukan pada Spark.

\item Bab 4 Analisis dan Perancangan\\
Berisi analisis masalah, diagram alur, \textit{use case} dan skenario, diagram kelas, dan perancangan antarmuka.

\item Bab 5 Implementasi dan Pengujian\\
Berisi implementasi antarmuka perangkat lunak, pengujian eksperimen, dan kesimpulan dari pengujian.

\item Bab 5 Implementasi dan Pengujian\\
Berisi kesimpulan awal sampai akhir penelitian dan saran untuk penelitian selanjutnya.


\end{itemize}


